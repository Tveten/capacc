---
title: "MVCAPA with correlations --- meeting 2"
date: October 02, 2019
output: 
  beamer_presentation
---

\include{./latexCommands}

# Overview
I have:

* Proved that optimising $\bJ$ is NP-hard.
* Reformulated problem to a binary quadratic program. Opens up wide range of literature.
* Found a DP solution to optimising $\bJ$ when the correlation matrix is AR(1).
* Written everything down in Latex.
* Looked briefly on the case where $\bSigma$ is block diagonal.

Next steps?

# Why is optimising J NP-hard?
Savings:

\(
  S(s, e, \bJ) = (e - s)\left( \sum_{j \in \bJ} a_{jj}\bar{x}_j^2 + 2 \sum_{j \in \bJ} \sum_{i < j \in \bJ} a_{ij} \bar{x}_i \bar{x}_j \right)
\)

Binary quadratic programs (BQP):

\(
  \underset{\bu \in \{ 0, 1 \}^p}{\max} \bu^T \mathbf{Q} \bu + \mathbf{c}^T \bu
\)

Reformulation to a BQP:
\begin{align*}
  &\underset{\bJ}{\max}\; [S(s, e, \bJ) - P(|\bJ|)] \\
  =& (e - s) \; \underset{\bu}{\max} \left[ \bu^T (\bA \circ \barx \barx^T) \bu - P(\| \bu \|_2^2) \right],
\end{align*}
where $P(\cdot)$ has to be linear in $\bu$.

BQPs are NP-hard, even when $\mathbf{Q}$ is positive definite.

# Look at BQP solvers?

- A very general solution.
- Alex emailed Adam Letchford, got some useful tips on BQPs.
- Hundreds/thousands of articles on the topic.
- Commercial solvers exist (CPLEX and Gurobi) together with R distributions -> Easy to implement if licence (Lancaster should have CPLEX, how to I access?).
- Implement and test?

# Dynamic program for spatial AR(1) structure
Let
\(
  B(d, k) = \underset{1 \leq i_1 < \ldots < i_k \leq d}{\max} S(\{ i_1, \ldots, i_k \})
\)

Then

\(
    B(d, k) = \max\left[B(d - 1, k), \underset{1 \leq j \leq k}{\max}\; B(d - j - 1, k - j) + S((d - j + 1):d)\right],
\)
for $0 \leq k \leq p$ and $k < d \leq p$, where $B(\cdot, 0) = 0$, and $B(k, k) = S((d - j + 1):d)$.

I.e.,

\(
  \underset{\bJ : |\bJ| = k}{\max} S(\bJ) = B(p, k).
\)

Unfortunately it is $O(p^3)$, but restrict $|\bJ| \leq l$ and it becomes $O(l^2p)$.

# Moreover...

- Two penalty regimes: Sparse, dense.
- In sparse, components are added one at a time.
- In dense, all components will be added because penalty is constant over $|\bJ|$.
- So in general it should be sufficient to optimise $\bJ$ over the sparse alternatives,
i.e., $|\bJ| \leq$ the boundary between the two regimes?
- Theoretically, when $|\bJ| / p \leq p^{-1/2} = a$ we are in sparse world, $>$ in dense.
- In practice, the transition between penalties occurs at $|\bJ| \geq \frac{p + 2\sqrt{p\psi}}{2\log{p}} = b > a$ (given the i.i.d. penalties).

# Tried to find pruning strategy
Does not look promising so far. Tips?

Reasoning: If $B(d - 2, 2) + S(\{ d \}) \leq B(d - 3, 1) + S(\{ d, d - 1 \})$,
do I know that $B(d - 2, l) - B(d - 3, l - 1) \leq  S(\{ d, d - 1 \}) - S(\{ d \})$ for
all $l \geq 2$?

# Block diagonal covariance matrix
\(
  (e - s) \underset{\bJ_1, \ldots, \bJ_m}{\max} \left[\sum_{i = 1}^m x_i(\bJ_i) A_i^{-1} x_i(\bJ_i) - P(\sum_{i = 1}^m |\bJ_i|) \right]
\)

- Optimise jointly.
- Optimise in parallel, then new penalty per block.
- Only assume sparsity between blocks, but not within.

# What now?

- Implement and test BQP?
- Try harder to find a pruning strategy for AR(1)?
- Try to generalise DP to AR(p)?
- Should I look at the equicorrelated case?
- Block diagonal covariance matrix?
- What about L1-penalised cost function?
- ...
