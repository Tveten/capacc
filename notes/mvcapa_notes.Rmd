---
title: "MVCAPA notes"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### MVCAPA notes

## Correlation simulation tests
a plots: These plots are included for comparisons with the correlation plots and
to show what the error rate is when no correlation is included.
Ideally, the highest a should have almost 100% error control, while the others
below 100%, to show that the threshold results in approximately 0.1% errors 
under a correct model.
If all a's result in no false alarms, then the used threshold (the higest
among the tried a's) result in much lower than 0.1% errors.
If prop = 0, K_hat should be 0, if prop > 0, K_hat should be 1.
The default threshold is also included to show that it does not control errors
well.

rho and phi plots: These plots show the effects of adding constant positive
correlation (rho) and an AR-type correlation matrix (phi) for a threshold
with a = 2 if 

# Scaling the penalty
Scale the proportionality constant of \( \psi \propto \log n \)?

* Easier to realate to the theory.

Or scale the entire set of penalties?

# Results
<!-- ![Here goes the caption.]{../images/} -->

## Optimizing the subset of affected components.
No shortcut to optimizing the subset J exactely for each (s, e)-pair.
Even for short-range AR(1) type spatial dependence.
Either restrict the size of J, look for approximate optimization or do something
completely different.
Restricting J to the subset sizes that fall within the sparse change setting 
makes sense because these are the changes where the sparsity is important to
handle explicitly. 
Another method that does not need combinatorial optimization can be used for the
dense changes.
An approximate optimization might be a more general solution.
Can an approximation be reached from looking at each of the p(p - 1)/2 terms 
separately?
